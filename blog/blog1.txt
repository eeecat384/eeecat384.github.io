标题：芯片（chip）认成薯片（chip）？国产AI抄袭了国外AI成果？

关键词：AI，LLM，技术博客

发布时间：2025-12-22

最近在网上看到一个说法：问AI芯片价格，他会给你薯片价格。这是因为芯片和薯片英文中都是chip，所以国产的AI其实是用的国外AI，只是把用户输入的中文翻译成英文，喂给AI，再把AI生成的英文翻译为中文，返回给用户，“芯片”错认为“薯片”就是翻译中产生的错误。

额，首先不说阴谋论和可操作性的事，先来看是不是有这么个现象。对此，我进行了一次小测试。

测试对象：  kimi k1.5（简称k1.5）（是否联网）；kimi k2（k2）（是否联网）；deepseek v3（v3）（是否联网）；deepseek r1（r1）（是否联网）；千问（qwen）（网页端直接测试）；豆包（db）（网页端直接测试）
测试使用提示词：1、一袋芯片多贵啊，想吃了；   2、一袋芯片多贵
测试结果：  
    提示词          一袋芯片多贵啊，想吃了               一袋芯片多贵
结果
给薯片价格          db、v3、v3联网、r1、GPT-4            v3、GPT-4
给芯片价格          k2联网、k1.5联网                     db、v3联网、r1联网、k2、k1.5、k2联网、k1.5联网
问是芯片还是薯片    qwen、r1联网、k2、k1.5               r1、qwen                   

注：我觉得比较难绷的是豆包和v3联网，豆包直接联网搜索了薯片，v3搜到的都是芯片，结果给我说“搜索结果主要都是关于电子半导体芯片的涨价行情。我理解你想问的，可能是能吃的‘薯片’。”

测试结果比较好的是kimi，不论是k2或者k1.5，都没有给我薯片价格，而且联网后也是唯一能避开薯片的。

这里每个模型只测试了一次或两次，可能有偏差，并不能作为权威的来源，如果读者感兴趣可以自行测试。

上面这些可以看出来，确实有这个现象，无独有偶，几年前有过类似的质疑，不过是图像生成领域。有个国产AI有生成图像的功能。有人测试却发现，让画总线（计算机概念，bus），结果画出来的是巴士（bus），于是推断出和文章首段类似的结论，对此我隐约觉得不对，却没找到证据反驳。
我会回答这三个问题：
1、为什么会出现这种现象？
2、能说明偷用了或抄袭了国外AI吗？
3、上面那个测试能说明AI的能力吗?

第一，我们要知道LLM是怎么运作的，如果不知道可以看我的上一篇博客（暂时烂尾了，嘻嘻），简单的说，你输入一句话后，每个token（可以简单理解为每个词）都会转化为一个的向量，然后将这些向量丢到一堆神经元中进行运算，最后出来一个新的向量，对应着一个token，输出这个token后，前面的token和新的token一起，再次运算一遍，就这么不断运算，生成下去，直到算出来这段话该结束了，输出终止符，一段话也就生成完了。

openAI团队做了这么一个研究，题目叫做《语言模型可以解释语言模型中的神经元》（language models can explain neurons in language models）https://openai.com/index/language-models-can-explain-neurons-in-language-models/

这个研究里，他们用GPT-4对GPT-2里的每个神经元对什么词活跃进行研究，发现了如第0层的第816个神经元对表达“正确”的词敏感，第1层的第651个神经元对过渡词和标点敏感。并把代码开源，鼓励社区进行研究。

然后是《多语言模型中的知识表征与知识共享》（Qualifying Knowledge and Knowledge Sharing in Multilin gual Models）文章研究了知识神经元（Knowledge Neurons, KNs），发现多语言模型中确实存在大量共享的KNs，在测试的10种语言中，KNs在不同语言之间有显著的重叠，表明知识表示在一定程度上是语言无关的。

所以第一个问题的答案是：AI可能在用英文数据训练时，某个神经元对“chip”这个词敏感，并“搞混了”薯片和芯片，导致了最后错误的理解。并且，你可能注意到了，我使用的提示词中，故意使用了“一袋”甚至“想吃”这种故意将AI往坑里带的词，如果提示词中加入“我是电气工程师”“一颗”等，给出的就都是正确的“芯片”价格了。

因此，我认为此类现象并不能有力地指控国产AI抄袭了国外的成果，而是使用英文数据训练导致的必然结果。这个测试也并不能反应AI的真实水平，最多只能反映训练数据的英文大致占比。

2025.12.15晚，收工。

注：本文提及资料可在我的github仓库中找到（github在国内大部分时间访问不了，如想访问请自寻方法）

又注：可以给我的github仓库点个收藏咩（期待
